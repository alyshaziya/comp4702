{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sn5zSNyN6UCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a13be4-50fa-4a4f-9c76-dd0bf2a02b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Mount google colab to drive to access to the dataset (uncomment if you use Google Colab + Drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7_HQqqP608G"
      },
      "source": [
        "# Q1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GdrCuq6_6z9_"
      },
      "outputs": [],
      "source": [
        "# TODO: Load dataset\n",
        "w3classif = pd.read_csv(\"/content/drive/MyDrive/w3classif.csv\", header = None) # Specify full path if you use Google Colab + Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GHGgsCKw7OQL"
      },
      "outputs": [],
      "source": [
        "# TODO: create a function to create 10 different shuffled train and test set pairs from the original dataset\n",
        "def create_train_test_data(test_size=0.3):\n",
        "  # For storing data\n",
        "  trains, tests = [], []\n",
        "\n",
        "  for i in range(10):\n",
        "    # TODO: Shuffle the dataset\n",
        "    shuffled_data = w3classif.sample(frac=1, random_state=i).reset_index(drop=True)\n",
        "\n",
        "    # TODO: Split the dataset\n",
        "    train_data, test_data = train_test_split(shuffled_data, test_size=test_size, random_state=i)\n",
        "\n",
        "    # Store data\n",
        "    trains.append(train_data)\n",
        "    tests.append(test_data)\n",
        "\n",
        "  return trains, tests\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFcjavtQ8Zp7"
      },
      "source": [
        "# Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C-xKodm78eHS"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# TODO: create a function to repeat classification with kNN for multiple trials\n",
        "def repeat_knn(trials=10, test_size=0.3):\n",
        "\n",
        "  # For storing\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  # Create train and test datasets (calling the function you created previously)\n",
        "  trains, tests = create_train_test_data(test_size)\n",
        "\n",
        "  for i in range(trials):\n",
        "    train_data = trains[i]\n",
        "    test_data = tests[i]\n",
        "\n",
        "    # TODO: Split train data into features and target\n",
        "    X_train = train_data[[0, 1]]\n",
        "    y_train = train_data[2]\n",
        "\n",
        "\n",
        "    # TODO: Split test data into features and target\n",
        "    X_test = test_data[[0, 1]]\n",
        "    y_test = test_data[2]\n",
        "\n",
        "\n",
        "    # TODO: Initialize the k-NN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "\n",
        "    # TODO: Train the classifier on the training data\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # TODO: Make predictions on the training and test data\n",
        "    y_train_pred = knn.predict(X_train)\n",
        "    y_test_pred = knn.predict(X_test)\n",
        "\n",
        "\n",
        "    # TODO: Calculate training and test accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "    # TODO: Calculate training and test loss (misclassification rate)\n",
        "    train_loss = 1 - train_accuracy\n",
        "    test_loss = 1 - test_accuracy\n",
        "\n",
        "\n",
        "    # TODO: Store train and test losses\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "\n",
        "  return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NqW6b8EY-NMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4b73bd-05d6-4faa-b3f1-3af48e328411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Training Loss (Misclassification Rate): 2.50%\n",
            "Avg Test Loss (Misclassification Rate): 4.58%\n"
          ]
        }
      ],
      "source": [
        "# Print the average training and test losses for 10 trials using the function implemented above\n",
        "train_losses, test_losses = repeat_knn(trials=10, test_size=0.3)\n",
        "print(f'Avg Training Loss (Misclassification Rate): {np.array(train_losses).mean() * 100:.2f}%')\n",
        "print(f'Avg Test Loss (Misclassification Rate): {np.array(test_losses).mean() * 100:.2f}%')\n",
        "\n",
        "\"\"\"\n",
        "Old results:\n",
        "Avg Training Loss (Misclassification Rate): 1.66%\n",
        "Avg Test Loss (Misclassification Rate): 3.93%\n",
        "Test/Train loss increased, likely due to model seeing different data each shuffle.\n",
        "Taking the average across 10 splits is more reliable. May have gotten lucky with the split before.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p4axUHW-mYY"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "et4o2flU-YdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10e3ce3-eac8-4fd3-a143-70ec259e5048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test size: 10% (90% training)\n",
            "Avg Training Loss: 2.61%\n",
            "Avg Test Loss: 5.25%\n",
            "\n",
            "Test size: 30% (70% training)\n",
            "Avg Training Loss: 2.50%\n",
            "Avg Test Loss: 4.58%\n",
            "\n",
            "Test size: 50% (50% training)\n",
            "Avg Training Loss: 2.80%\n",
            "Avg Test Loss: 4.75%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define all possible test set sizes\n",
        "test_sizes = [0.1, 0.3, 0.5] # 90/10, 70/30, 50/50\n",
        "\n",
        "for ts in test_sizes:\n",
        "  # TODO: Repeat Q1 and Q2 and print the average loss for 10 trials for each test set size\n",
        "  print(f\"\\nTest size: {int(ts * 100)}% ({int((1 - ts) * 100)}% training)\")\n",
        "\n",
        "  train_losses, test_losses = repeat_knn(trials=10, test_size=ts)\n",
        "\n",
        "  print(f\"Avg Training Loss: {np.mean(train_losses) * 100:.2f}%\")\n",
        "  print(f\"Avg Test Loss: {np.mean(test_losses) * 100:.2f}%\")\n",
        "\n",
        "  # Main findings\n",
        "  # Less training data means worse generalisation\n",
        "\n",
        "  # 90/10 split\n",
        "  # train loss: is higher bc more data to learn from\n",
        "  # test loss: small test set = more variance.\n",
        "\n",
        "  #70/30 split (maybe a sweet spot)\n",
        "  # Train/test loss is balanced due to the split\n",
        "\n",
        "  #50/50 split\n",
        "  # Train loss: slightly worse, less data to learn from\n",
        "  # Test loss: slightly higher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xClMuw8A_BWF"
      },
      "source": [
        "# Q4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bj_HNZyV_KTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9348c209-f567-4b1c-e95d-41eb3c38983a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test size: 10% (90% training)\n",
            "Training Loss Std Dev: 0.4182%\n",
            "Testing Loss Std Dev: 3.9878%\n",
            "\n",
            "Test size: 30% (70% training)\n",
            "Training Loss Std Dev: 0.9221%\n",
            "Testing Loss Std Dev: 2.1960%\n",
            "\n",
            "Test size: 50% (50% training)\n",
            "Training Loss Std Dev: 1.1106%\n",
            "Testing Loss Std Dev: 0.9204%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define all possible test set sizes\n",
        "test_sizes = [0.1, 0.3, 0.5] # 90/10, 70/30, 50/50\n",
        "\n",
        "for ts in test_sizes:\n",
        "  # TODO: Calculate the sample standard deviation of your training and test set error values over the 10 trials from Q2 and Q3.\n",
        "  print(f\"\\nTest size: {int(ts * 100)}% ({int((1 - ts) * 100)}% training)\")\n",
        "\n",
        "  # Repeat the 10 trials for this test size\n",
        "  train_losses, test_losses = repeat_knn(trials=10, test_size=ts)\n",
        "\n",
        "  # TODO: Calculate the sample standard deviation of training and test losses\n",
        "  train_std = np.std(train_losses, ddof=1)\n",
        "  test_std = np.std(test_losses, ddof=1)\n",
        "\n",
        "  print(f\"Training Loss Std Dev: {train_std * 100:.4f}%\")\n",
        "  print(f\"Testing Loss Std Dev: {test_std * 100:.4f}%\")\n",
        "\n",
        "  #Observation\n",
        "  # 90/10 - very stable but test results vary a lot due to small test set\n",
        "  # 70/30 - both test/train are moderate (seems balanced)\n",
        "  # 50/50 - training varies more (less data) but test set is large so more stable variation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrYcoSg__jdY"
      },
      "source": [
        "# Q5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bNkpYItl_Oh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8460873f-ae09-4973-de0e-a141f0fc37b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-Fold Cross-Validation Mean Error: 5.00%\n",
            "10-Fold Cross-Validation Std Dev: 4.86%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# TODO: Shuffle the original dataset\n",
        "data_classif = w3classif.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# TODO: Split the dataset into features and target\n",
        "X = data_classif[[0, 1]]  # first two columns\n",
        "y = data_classif[2]       # third column\n",
        "\n",
        "# TODO: Initialize the k-NN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# TODO: Define the number of folds for cross-validation\n",
        "num_folds = 10\n",
        "\n",
        "# TODO: Perform cross-validation\n",
        "cv_scores = cross_val_score(knn, X, y, cv=num_folds, scoring='accuracy')\n",
        "\n",
        "\n",
        "# TODO: Calculate mean and standard deviation of cross-validation error\n",
        "cv_errors = 1 - cv_scores  # convert accuracy to misclassification\n",
        "mean_error = np.mean(cv_errors)\n",
        "std_error = np.std(cv_errors, ddof=1)\n",
        "\n",
        "\n",
        "# TODO: Print the errors\n",
        "print(f\"10-Fold Cross-Validation Mean Error: {mean_error * 100:.2f}%\")\n",
        "print(f\"10-Fold Cross-Validation Std Dev: {std_error * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}